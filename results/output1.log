torchrun --nproc_per_node 4 --nnodes 2 --node_rank 1 --master_addr 10.3.0.26 --master_port 32364 pretrain_megatron_llama.py --save /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000 --train-data-path /gfshome/llama2-datasets/wudao_llamabpe_text_document --lr 1e-5 --min-lr 1e-6 --lr-decay-style linear --adam-beta1 0.9 --adam-beta2 0.95 --weight-decay 0.1 --clip-grad 1.0 --init-method-std 0.006 --lr-decay-iters 40 --lr-warmup-iters 9 --train-iters 40 --micro-batch-size 1 --global-batch-size 8 --num-layers 32 --hidden-size 4096 --num-attention-heads 32 --ffn-hidden-size 11008 --seq-length 128 --max-position-embeddings 4096 --max-padding-length 128 --log-interval 1 --eval-interval 10000 --eval-iters 10 --save-interval 100000 --tensorboard-queue-size 1 --tensorboard-dir /mnt/output_megatron_llama2/tensorboard/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000_2024.11.30-16.22.48 --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensor-model-parallel-size 1 --pipeline-model-parallel-size 4 --no-load-optim --no-load-rng --num-workers 0 --seed 1234 --max-padding-length 128 --extra-vocab-size 0 --patch-tokenizer-type LLamaTokenizer --dataset LLama-Pretrain-Idxmap --swiglu --normalization RMSNorm --use-llama2-rotary-position-embeddings --position-embedding-type rope --untie-embeddings-and-output-weights --disable-bias-linear --forward-backward-disaggregating --bf16 --load /gfshome/llama2-ckpts/Llama-2-7b-hf-to-megatron-tp1-pp4 --recompute-activations --use-distributed-optimizer
> setting tensorboard ...
 > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 1750142976
 > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 1619066880
Rank level:Rank level:  0 03 
2
Rank level: 0 2
Rank level: 0 3
(min, max) time across ranks (ms):
    load-checkpoint ................................: (9629.31, 9629.81)
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (12789.67, 12832.52)
    train/valid/test-data-iterators-setup ..........: (1731.72, 2762.55)
 iteration        1/      40 | consumed samples:            8 | elapsed time per iteration (ms): 3164.5 | learning rate: 1.111E-06 | global batch size:     8 | lm loss: 1.068049E+01 | loss scale: 1.0 | grad norm: 47.611 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 4] (after 1 iterations) memory (MB) | allocated: 18554.880859375 | max allocated: 18554.88427734375 | reserved: 18928.0 | max reserved: 18928.0
[Rank 6] (after 1 iterations) memory (MB) | allocated: 20054.9287109375 | max allocated: 20054.93212890625 | reserved: 20642.0 | max reserved: 20642.0
 iteration        2/      40 | consumed samples:           16 | elapsed time per iteration (ms): 1621.3 | learning rate: 2.222E-06 | global batch size:     8 | lm loss: 1.061451E+01 | loss scale: 1.0 | grad norm: 204.464 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        3/      40 | consumed samples:           24 | elapsed time per iteration (ms): 1578.4 | learning rate: 3.333E-06 | global batch size:     8 | lm loss: 9.666226E+00 | loss scale: 1.0 | grad norm: 59.516 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        4/      40 | consumed samples:           32 | elapsed time per iteration (ms): 1599.8 | learning rate: 4.444E-06 | global batch size:     8 | lm loss: 1.067665E+01 | loss scale: 1.0 | grad norm: 70.072 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        5/      40 | consumed samples:           40 | elapsed time per iteration (ms): 1499.4 | learning rate: 5.556E-06 | global batch size:     8 | lm loss: 1.098933E+01 | loss scale: 1.0 | grad norm: 46.676 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        6/      40 | consumed samples:           48 | elapsed time per iteration (ms): 1497.7 | learning rate: 6.667E-06 | global batch size:     8 | lm loss: 9.861741E+00 | loss scale: 1.0 | grad norm: 41.058 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        7/      40 | consumed samples:           56 | elapsed time per iteration (ms): 1528.5 | learning rate: 7.778E-06 | global batch size:     8 | lm loss: 9.623478E+00 | loss scale: 1.0 | grad norm: 68.703 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        8/      40 | consumed samples:           64 | elapsed time per iteration (ms): 1505.1 | learning rate: 8.889E-06 | global batch size:     8 | lm loss: 8.693103E+00 | loss scale: 1.0 | grad norm: 29.727 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration        9/      40 | consumed samples:           72 | elapsed time per iteration (ms): 1492.0 | learning rate: 1.000E-05 | global batch size:     8 | lm loss: 7.941988E+00 | loss scale: 1.0 | grad norm: 20.591 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       10/      40 | consumed samples:           80 | elapsed time per iteration (ms): 1522.8 | learning rate: 9.710E-06 | global batch size:     8 | lm loss: 7.846517E+00 | loss scale: 1.0 | grad norm: 53.243 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       11/      40 | consumed samples:           88 | elapsed time per iteration (ms): 1487.8 | learning rate: 9.419E-06 | global batch size:     8 | lm loss: 8.174131E+00 | loss scale: 1.0 | grad norm: 27.915 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       12/      40 | consumed samples:           96 | elapsed time per iteration (ms): 1565.8 | learning rate: 9.129E-06 | global batch size:     8 | lm loss: 7.454684E+00 | loss scale: 1.0 | grad norm: 17.091 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       13/      40 | consumed samples:          104 | elapsed time per iteration (ms): 1599.9 | learning rate: 8.839E-06 | global batch size:     8 | lm loss: 6.977566E+00 | loss scale: 1.0 | grad norm: 10.188 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       14/      40 | consumed samples:          112 | elapsed time per iteration (ms): 1500.0 | learning rate: 8.548E-06 | global batch size:     8 | lm loss: 6.573184E+00 | loss scale: 1.0 | grad norm: 9.896 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       15/      40 | consumed samples:          120 | elapsed time per iteration (ms): 1531.1 | learning rate: 8.258E-06 | global batch size:     8 | lm loss: 6.718001E+00 | loss scale: 1.0 | grad norm: 10.394 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       16/      40 | consumed samples:          128 | elapsed time per iteration (ms): 1498.4 | learning rate: 7.968E-06 | global batch size:     8 | lm loss: 6.314437E+00 | loss scale: 1.0 | grad norm: 27.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       17/      40 | consumed samples:          136 | elapsed time per iteration (ms): 1498.5 | learning rate: 7.677E-06 | global batch size:     8 | lm loss: 6.541853E+00 | loss scale: 1.0 | grad norm: 12.488 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       18/      40 | consumed samples:          144 | elapsed time per iteration (ms): 1571.4 | learning rate: 7.387E-06 | global batch size:     8 | lm loss: 6.063620E+00 | loss scale: 1.0 | grad norm: 7.765 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       19/      40 | consumed samples:          152 | elapsed time per iteration (ms): 1529.4 | learning rate: 7.097E-06 | global batch size:     8 | lm loss: 6.139963E+00 | loss scale: 1.0 | grad norm: 6.945 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       20/      40 | consumed samples:          160 | elapsed time per iteration (ms): 1480.5 | learning rate: 6.806E-06 | global batch size:     8 | lm loss: 6.304391E+00 | loss scale: 1.0 | grad norm: 6.987 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       21/      40 | consumed samples:          168 | elapsed time per iteration (ms): 1490.0 | learning rate: 6.516E-06 | global batch size:     8 | lm loss: 6.230421E+00 | loss scale: 1.0 | grad norm: 23.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       22/      40 | consumed samples:          176 | elapsed time per iteration (ms): 1497.5 | learning rate: 6.226E-06 | global batch size:     8 | lm loss: 6.054504E+00 | loss scale: 1.0 | grad norm: 5.185 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       23/      40 | consumed samples:          184 | elapsed time per iteration (ms): 1502.8 | learning rate: 5.935E-06 | global batch size:     8 | lm loss: 5.995919E+00 | loss scale: 1.0 | grad norm: 8.430 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       24/      40 | consumed samples:          192 | elapsed time per iteration (ms): 1525.3 | learning rate: 5.645E-06 | global batch size:     8 | lm loss: 5.700890E+00 | loss scale: 1.0 | grad norm: 7.242 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       25/      40 | consumed samples:          200 | elapsed time per iteration (ms): 1411.9 | learning rate: 5.355E-06 | global batch size:     8 | lm loss: 5.989684E+00 | loss scale: 1.0 | grad norm: 7.073 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       26/      40 | consumed samples:          208 | elapsed time per iteration (ms): 1582.2 | learning rate: 5.065E-06 | global batch size:     8 | lm loss: 5.842979E+00 | loss scale: 1.0 | grad norm: 6.383 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       27/      40 | consumed samples:          216 | elapsed time per iteration (ms): 1481.1 | learning rate: 4.774E-06 | global batch size:     8 | lm loss: 5.835032E+00 | loss scale: 1.0 | grad norm: 5.210 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       28/      40 | consumed samples:          224 | elapsed time per iteration (ms): 1497.9 | learning rate: 4.484E-06 | global batch size:     8 | lm loss: 5.939836E+00 | loss scale: 1.0 | grad norm: 7.354 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       29/      40 | consumed samples:          232 | elapsed time per iteration (ms): 1451.9 | learning rate: 4.194E-06 | global batch size:     8 | lm loss: 5.882034E+00 | loss scale: 1.0 | grad norm: 5.168 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       30/      40 | consumed samples:          240 | elapsed time per iteration (ms): 1548.8 | learning rate: 3.903E-06 | global batch size:     8 | lm loss: 6.088678E+00 | loss scale: 1.0 | grad norm: 6.036 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       31/      40 | consumed samples:          248 | elapsed time per iteration (ms): 1524.6 | learning rate: 3.613E-06 | global batch size:     8 | lm loss: 5.971962E+00 | loss scale: 1.0 | grad norm: 5.500 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       32/      40 | consumed samples:          256 | elapsed time per iteration (ms): 1572.5 | learning rate: 3.323E-06 | global batch size:     8 | lm loss: 5.816546E+00 | loss scale: 1.0 | grad norm: 6.109 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       33/      40 | consumed samples:          264 | elapsed time per iteration (ms): 1527.4 | learning rate: 3.032E-06 | global batch size:     8 | lm loss: 5.808424E+00 | loss scale: 1.0 | grad norm: 20.825 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       34/      40 | consumed samples:          272 | elapsed time per iteration (ms): 1512.1 | learning rate: 2.742E-06 | global batch size:     8 | lm loss: 6.027952E+00 | loss scale: 1.0 | grad norm: 4.564 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       35/      40 | consumed samples:          280 | elapsed time per iteration (ms): 1562.7 | learning rate: 2.452E-06 | global batch size:     8 | lm loss: 5.965931E+00 | loss scale: 1.0 | grad norm: 32.837 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       36/      40 | consumed samples:          288 | elapsed time per iteration (ms): 1546.5 | learning rate: 2.161E-06 | global batch size:     8 | lm loss: 5.818915E+00 | loss scale: 1.0 | grad norm: 8.314 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       37/      40 | consumed samples:          296 | elapsed time per iteration (ms): 1480.1 | learning rate: 1.871E-06 | global batch size:     8 | lm loss: 5.948098E+00 | loss scale: 1.0 | grad norm: 6.148 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       38/      40 | consumed samples:          304 | elapsed time per iteration (ms): 1506.0 | learning rate: 1.581E-06 | global batch size:     8 | lm loss: 5.833479E+00 | loss scale: 1.0 | grad norm: 5.744 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       39/      40 | consumed samples:          312 | elapsed time per iteration (ms): 1521.5 | learning rate: 1.290E-06 | global batch size:     8 | lm loss: 5.825675E+00 | loss scale: 1.0 | grad norm: 8.025 | number of skipped iterations:   0 | number of nan iterations:   0 |
 iteration       40/      40 | consumed samples:          320 | elapsed time per iteration (ms): 1543.1 | learning rate: 1.000E-06 | global batch size:     8 | lm loss: 6.100275E+00 | loss scale: 1.0 | grad norm: 14.535 | number of skipped iterations:   0 | number of nan iterations:   0 |
Rank 4 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_002/distrib_optim.pt
Rank 6 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_003/distrib_optim.pt
Rank 4 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_002/nondistrib_optim.pt
Rank 4 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_002/model_optim_rng.pt
Rank 6 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_003/nondistrib_optim.pt
Rank 6 will save checkpoint to /mnt/output_megatron_llama2/checkpoint/dsw-pretrain-megatron-gpt3-7B-lr-1e-5-bs-1-seqlen-128-pr-bf16-tp-1-pp-4-ac-sel-do-true-sp-false-tt-40960-wt-10000/iter_0000040/mp_rank_00_003/model_optim_rng.pt
----------------------------------------------------------------------------------------------------------------
 validation loss at iteration 40 on validation set | lm loss value: 5.379282E+00 | lm loss PPL: 2.168664E+02 | 
----------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------
 validation loss at iteration 40 on test set | lm loss value: 5.277415E+00 | lm loss PPL: 1.958630E+02 | 
----------------------------------------------------------------------------------------------------------
P2P   : 671139840
Reduce: 388576207888
Gather: 9714401520
Broadcast: 553064
Consumed: 4888320
P2P   : 419462400
Reduce: 420034917328
Gather: 10500858096
Broadcast: 553064
Consumed: 4888320
P2P   : 671139840
Reduce: 388576207888
Gather: 9714401520
Broadcast: 553064
Consumed: 4888320
P2P   : 419462400
Reduce: 420034917328
Gather: 10500858096
Broadcast: 553064
Consumed: 4888320
